{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be97fdca",
   "metadata": {},
   "source": [
    "tokenizador eh basicamente quebrar palavras em pedaços menores, chamados tokens.\n",
    "\n",
    "1. vamos preparar a entrada de textos para treinar as llms\n",
    "como prepara?\n",
    "\n",
    "primeiro passo: dividir o texto em letras invididuais e  subword tokens\n",
    "\n",
    "segundo passo: coverter tokens em tokens ids\n",
    "\n",
    "terceiro passo: encode tokens ids em representação de vetores\n",
    "\n",
    "exemplo\n",
    "\n",
    "this is an example -> texto tokenizado [this | is | an | example] -> token ids [ 4013 | 201 | 302 | 1134] -> token embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5984eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# passo 1 criar tokens\n",
    "with open('dataset\\Dataset1.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read().replace('\\n', ' ')\n",
    "\n",
    "print(f\"Total de caracteres: {len(raw_text)}\")\n",
    "print(f\"{raw_text[:100]}...\")\n",
    "\n",
    "# dividir todo o texto em palavras\n",
    "import re\n",
    "\n",
    "preprocessed = re.split(r'[*,.:;?!\\-()\"\\'\\s]|--', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(f\"Total de palavras: {len(preprocessed)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50913b42",
   "metadata": {},
   "source": [
    "# passo 2 criar token ids\n",
    "agora vamos criar um vocabulario unico\n",
    "um vocabulario eh uma lista de todas as palavras em um corpus de texto\n",
    "vamos criala de modo alfabetico\n",
    "\n",
    "tokens unicos = id unicos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(f\"Tamanho do vocabulario: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf45325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos criar o vocabulario\n",
    "# cada elemento tem seu id\n",
    "# encoder seria buscar o id pelo token\n",
    "# decoder seria o inverso do encoder, vamos buscar um token pelo id\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "#salvar vocabulario em um arquivo\n",
    "import json\n",
    "with open('vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        # s = token\n",
    "        # i = token id\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'[*,.:;?!\\-()\"\\'\\s]|--', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = ' '.join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([.,!?\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628007ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teste de encode do tokenizer\n",
    "tokenizer = tokenizerV1(vocab)\n",
    "text = 'Amadurecido pela leitura atenta dos teóricos da linguagem'\n",
    "ids = tokenizer.encode(text)\n",
    "print(f\"Token ids: {ids}\")\n",
    "# Token ids: [1191, 21335, 18304, 7977, 12964, 26278, 11456, 18532]\n",
    "\n",
    "# teste de decode do tokenizer\n",
    "print(f\"Decoded text: {tokenizer.decode(ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b428a873",
   "metadata": {},
   "source": [
    "caso nao tenha o token no vocabulario, o encode vai gerar um erro\n",
    "para resolver isso, podemos adicionar um token especial <unk> (unknown)\n",
    "e vamos adicionar um endoftext token <eot> (end of text) para indicar o fim do texto no vocabulario ele basicamante vai ser o ultimo token do vocabulario para indicar o fim do texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        # s = token\n",
    "        # i = token id\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'[*,.:;?!\\-()\"\\'\\s]|--', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = ' '.join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([.,!?\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teste de encode do tokenizer\n",
    "tokenizer = tokenizerV2(vocab)\n",
    "text = 'Olá meu amigo, como você está?'\n",
    "ids = tokenizer.encode(text)\n",
    "print(f\"Token ids: {ids}\")\n",
    "\n",
    "# teste de decode do tokenizer\n",
    "print(f\"Decoded text: {tokenizer.decode(ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ace3e3",
   "metadata": {},
   "source": [
    "conteudo adicional:\n",
    "\n",
    "[BOS] = begin of sentence: indica o inicio de uma sentença, que significa que o modelo deve começar a gerar texto a partir desse ponto.\n",
    "\n",
    "[SEP] = separator: usado para separar diferentes partes de um texto, como perguntas e respostas em um diálogo.\n",
    "\n",
    "[PAD] = padding: usado para preencher sequências de texto para que todas tenham o mesmo comprimento em um lote (batch) de dados.\n",
    "\n",
    "[CLS] = classification: usado em tarefas de classificação de texto, onde o modelo precisa classificar uma sentença ou um par de sentenças.\n",
    "\n",
    "[EOS] = end of sentence: indica o fim de uma sentença, semelhante ao <|endoftext|> que usamos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557ab6b",
   "metadata": {},
   "source": [
    "O GPT utiliza um tokenizador baseado em Byte Pair Encoding (BPE), que é uma técnica de tokenização subword. O BPE divide palavras em subunidades menores, permitindo que o modelo lide melhor com palavras raras ou desconhecidas. Além disso, o tokenizador do GPT inclui tokens especiais como <|endoftext|> para indicar o fim do texto, o que é crucial para o funcionamento do modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6c890",
   "metadata": {},
   "source": [
    "Byte Pair Encoding \n",
    "\n",
    "Algoritimos de tokenização\n",
    "- word based\n",
    "- subword based\n",
    "- character based\n",
    "\n",
    "# word based\n",
    "eh basicamente o que fiz a cima, o problema eh, o que fazer com palavras desconhecidas?\n",
    "\n",
    "# character based\n",
    "cada caractere eh um token, o problema eh que a sequencia de tokens fica muito grande mas resolve o problema de palavras desconhecidas\n",
    "- perde o significado das palavras\n",
    "- a sequencia de tokens fica muito grande\n",
    "\n",
    "# subword based\n",
    "combina o melhor dos dois mundos\n",
    "- nao divide palavras usadas com frequencia em caracteres melhores\n",
    "- divide palavras raras em caracteres ou subwords menores\n",
    "exemplo:\n",
    "boys -> ['boy', 's']\n",
    "\n",
    "ele ajuda o modelo a aprender que diferentes palavras vem da mesma raiz -> token, tokens e tokenization teriam significado similar\n",
    "\n",
    "ele ajuda o modelo a entender que tokenization e modernization tem raizes diferentes porem compartilham o sufixo 'ization' e são usadas nas mesmas situacoes sintaticas\n",
    "\n",
    "BPE -> Byte Pair Encoding\n",
    "Algoritmo BPE (1994) -> um algoritmo de compressão de dados que pode ser adaptado para tokenização de texto. \n",
    "Ele funciona substituindo pares de bytes mais frequentes por um byte único que não aparece no texto original. \n",
    "No contexto da tokenização, o BPE começa com um vocabulário inicial de caracteres individuais e itera para combinar os pares de tokens mais frequentes em novos tokens, criando assim um vocabulário de subwords.\n",
    "\n",
    "exemplo de algoritmo BPE:\n",
    "original data: \"aaabdaaabac\"\n",
    "1. contar pares de bytes:\n",
    "   aa: 4\n",
    "   ab: 2\n",
    "   ba: 2\n",
    "   ad: 1\n",
    "   da: 1\n",
    "   ac: 1\n",
    "2. substituir o par mais frequente (aa) por um novo token (X):\n",
    "   XabdXabac\n",
    "3. repetir o processo:\n",
    "   contar pares:\n",
    "    ab: 2\n",
    "    ba: 2\n",
    "    Xb: 2\n",
    "    ad: 1\n",
    "    da: 1\n",
    "    ac: 1\n",
    "    substituir o par mais frequente (ab) por um novo token (Y):\n",
    "    XYdXYac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266fa00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import importlib\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = ('hello, do you like tea? <|endoftext|> In the sunlit terraces' 'of sumeunknowPlace')\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
